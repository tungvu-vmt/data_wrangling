{"cells":[{"cell_type":"markdown","metadata":{"id":"S8tinZOUlDER"},"source":["<div class=\"alert alert-block alert-danger\">\n","\n","# FIT5196 Task 2 in Assessment 1\n","    \n","#### Student Name: Manh Tung Vu, Ilya Bessonov\n","\n","#### Student ID: 30531438, 34466029\n","\n","Date: 24/08/2024\n","\n","Environment: Python 3\n","\n","Libraries used:\n","* os (for interacting with the operating system, included in Python xxxx)\n","* pandas 2.1.4 (for dataframe, installed and imported)\n","* itertools (for performing operations on iterables)\n","* nltk 3.5 (Natural Language Toolkit, installed and imported)\n","* nltk.tokenize (for tokenization, installed and imported)\n","* nltk.stem (for stemming the tokens, installed and imported)\n","\n","    </div>"]},{"cell_type":"markdown","metadata":{"id":"xnnLnFnLlDEU"},"source":["<div class=\"alert alert-block alert-info\">\n","    \n","## Table of Contents\n","\n","</div>\n","\n","[1. Introduction](#Intro) <br>\n","[2. Importing Libraries](#libs) <br>\n","[3. Examining Input File](#examine) <br>\n","[4. Loading and Parsing Files](#load) <br>\n","$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n","$\\;\\;\\;\\;$[4.2. Whatever else](#whetev) <br>\n","$\\;\\;\\;\\;$[4.3. Genegrate numerical representation](#whetev1) <br>\n","[5. Writing Output Files](#write) <br>\n","$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n","$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n","[6. Summary](#summary) <br>\n","[7. References](#Ref) <br>"]},{"cell_type":"markdown","metadata":{"id":"z8mo6PPRlDEU"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ewZrff73lDEV"},"source":["In this notebook, we preprocess Google Reviews data to generate a vocabulary list and numerical representations, which will be used in downstream NLP tasks such as recommender systems, information retrieval, and machine translation. The data comes from businesses with at least 70 text reviews. Our primary goal is to extract meaningful features that will aid in understanding patterns in the text.\n","\n","The input data consists of two files:\n","\n","A CSV file containing business-related information, including:\n","\n","* gmap_id: The ID of the business.\n","* review_count: The total number of reviews for the business.\n","* review_text_count: The number of reviews that contain text.\n","* response_count: The number of responses from the business.\n","\n","A JSON file containing detailed review data for each business, including:\n","\n","* gmap_id: The ID of the business.\n","* reviews: A list of reviews, each containing:\n","  - user_id: The ID of the reviewer.  \n","  - time: The time of the review in UTC format (YYYY-MM-DD tt:hh\n",").\n","  - review_rating: The rating of the business.\n","  * review_text: The English review text. If the review is in another language, only the English translation is extracted, and emojis are removed. The text is normalized to lowercase.\n","  * If_pic: Whether the reviewer included pictures (Y/N).\n","  * pic_dim: The dimensions of any pictures included, as a list of tuples (e.g., [[h,w],[h,w]...]). An empty list is used if there are no pictures.\n","  * If_response: Whether the review has a response (Y/N).\n","* earliest_review_date: The earliest review date for the business in the given data subset (UTC format).\n","* latest_review_date: The latest review date for the business in the given data subset (UTC format).\n","\n","These files serve as the foundation for extracting and processing the text data, enabling us to build a structured vocabulary and numerical representation for further analysis."]},{"cell_type":"markdown","metadata":{"id":"bSr_kwKclDEV"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"]},{"cell_type":"markdown","metadata":{"id":"acwZw2NklDEW"},"source":["In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n","\n","* **os:** to interact with the operating system, e.g. navigate through folders to read files\n","* **re:** to define and use regular expressions\n","* **pandas:** to work with dataframes\n","* **itertools**: provides tools for creating iterators for efficient looping\n","* **nltk** (Natural Language Toolkit): to work with human text data\n","* **nltk.tokenize**: tools to split text into tokens\n","* **nltk.stem**: provides algorithm for stemming (reducing words to their root forms)\n","* **json**: to handle json (JavaScript Object Notation) data\n","* **sklearn.feature_extraction.text**: use \"CountVectorizer\" module to converts a collection of text documents to a matrix of token counts"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qgmGWs8HlDEW","executionInfo":{"status":"ok","timestamp":1725020835282,"user_tz":-180,"elapsed":4763,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"outputs":[],"source":["import os\n","import re\n","import pandas as pd\n","import json\n","\n","from itertools import chain\n","\n","import nltk\n","from nltk.probability import *\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.tokenize import MWETokenizer\n","from nltk.stem import PorterStemmer\n","from nltk.util import ngrams\n","from nltk import bigrams\n","\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"markdown","metadata":{"id":"lwNp0KnWlDEX"},"source":["-------------------------------------"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"SA7fSJiRlDEY"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bPCuEl8smTHW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725020856753,"user_tz":-180,"elapsed":21474,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"31b721b7-e094-489a-96f8-b2bdc57da55d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RyLbqkRxmCEZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725020857533,"user_tz":-180,"elapsed":782,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"ac909ee4-49ed-42db-c6a2-b46b4601440c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(176, 4)"]},"metadata":{},"execution_count":3}],"source":["# Load data\n","path = '/content/drive/MyDrive/FIT5196_assignment_files/'\n","summary_df = pd.read_csv(path + 'task1_044.csv') #('/content/drive/MyDrive/FIT5196_assignment_files/task1_044.csv')\n","summary_df.shape"]},{"cell_type":"code","source":["summary_df.dtypes"],"metadata":{"id":"XBRFQNbZ31mE","colab":{"base_uri":"https://localhost:8080/","height":209},"executionInfo":{"status":"ok","timestamp":1725020857534,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"a9e5e647-67d4-4c29-8aa5-20102a665c9b"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["gmap_id              object\n","review_count          int64\n","review_text_count     int64\n","response_count        int64\n","dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>gmap_id</th>\n","      <td>object</td>\n","    </tr>\n","    <tr>\n","      <th>review_count</th>\n","      <td>int64</td>\n","    </tr>\n","    <tr>\n","      <th>review_text_count</th>\n","      <td>int64</td>\n","    </tr>\n","    <tr>\n","      <th>response_count</th>\n","      <td>int64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["summary_df.describe()"],"metadata":{"id":"f2CxFY-m32Ob","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1725020857534,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"9109722c-afcf-4447-b998-b06cc483d153"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       review_count  review_text_count  response_count\n","count    176.000000         176.000000      176.000000\n","mean     214.238636         131.698864       32.312500\n","std      317.106940         194.234954       96.145182\n","min       51.000000          17.000000        0.000000\n","25%       74.000000          50.000000        0.000000\n","50%      108.000000          73.000000        0.000000\n","75%      199.000000         130.500000       23.500000\n","max     2853.000000        1776.000000      904.000000"],"text/html":["\n","  <div id=\"df-0c0bc4a4-ed3f-4999-955d-a14582c73db9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_count</th>\n","      <th>review_text_count</th>\n","      <th>response_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>176.000000</td>\n","      <td>176.000000</td>\n","      <td>176.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>214.238636</td>\n","      <td>131.698864</td>\n","      <td>32.312500</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>317.106940</td>\n","      <td>194.234954</td>\n","      <td>96.145182</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>51.000000</td>\n","      <td>17.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>74.000000</td>\n","      <td>50.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>108.000000</td>\n","      <td>73.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>199.000000</td>\n","      <td>130.500000</td>\n","      <td>23.500000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>2853.000000</td>\n","      <td>1776.000000</td>\n","      <td>904.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c0bc4a4-ed3f-4999-955d-a14582c73db9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0c0bc4a4-ed3f-4999-955d-a14582c73db9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0c0bc4a4-ed3f-4999-955d-a14582c73db9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0810d93d-d957-409e-aee5-544833bfb010\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0810d93d-d957-409e-aee5-544833bfb010')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0810d93d-d957-409e-aee5-544833bfb010 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"summary_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"review_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 954.9890877928729,\n        \"min\": 51.0,\n        \"max\": 2853.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          214.23863636363637,\n          108.0,\n          176.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_text_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 592.037365214389,\n        \"min\": 17.0,\n        \"max\": 1776.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          131.69886363636363,\n          73.0,\n          176.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 309.21958849621006,\n        \"min\": 0.0,\n        \"max\": 904.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          176.0,\n          32.3125,\n          904.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["ilya_path = '/content/drive/MyDrive/Assigment1data/task1_044.json'\n","tung_path = '/content/drive/MyDrive/FIT5196_assignment_files/task1_044.json'\n","with open('/content/drive/MyDrive/FIT5196_assignment_files/task1_044.json', 'r') as file:\n","  data = json.load(file)"],"metadata":{"id":"yQKxk4GIaAV5","executionInfo":{"status":"ok","timestamp":1725020864559,"user_tz":-180,"elapsed":1410,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7CJDLDI6lDEY"},"source":["Let's examine what is the content of the file. For this purpose, ...."]},{"cell_type":"code","source":["type(data)"],"metadata":{"id":"tTyABlONmEbO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725020866301,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"69212147-fd22-48ba-f80b-93a2e1271a7e"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"OpDVyW4YlDEZ"},"source":["It is noticed that the data is a dictionary. Now, we will further examine the structure of the data by checking its keys."]},{"cell_type":"code","source":["# Examine the structure of json file\n","gmap_list = list(data.keys())\n","print(len(gmap_list))\n","gmap_list[:10]"],"metadata":{"id":"FqQnqbX-aL0L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725020872723,"user_tz":-180,"elapsed":404,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"f91806f3-352f-4a4c-f142-59e2443c5c24"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["176\n"]},{"output_type":"execute_result","data":{"text/plain":["['0x54d4000810cde343:0x144f867433fa0966',\n"," '0x808164de5730862d:0xcb473d5d65fb1d39',\n"," '0x808327ba5f197e6d:0x5e31fb9492a334e7',\n"," '0x8084047107a176cb:0x889336c7010bad47',\n"," '0x808437fdd8593cbb:0x813a0531fbb5eb48',\n"," '0x80843e0cb60c8a03:0x32689617e649dd',\n"," '0x808447e3edf965c9:0x3f8efa82b45b0b45',\n"," '0x8084acae2f9d2541:0xe64c0a74ef6732af',\n"," '0x8084d092a8afc95d:0x1b7c5c974eb7e63f',\n"," '0x8084d0e3d7fa160f:0xf481ebe2fa126e60']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["As we know from Task 1, our json file has a structure:\n","* gmap_id: The ID of the business.\n","* reviews: A list of reviews, each containing:\n","  - user_id: The ID of the reviewer.  \n","  - time: The time of the review in UTC format (YYYY-MM-DD tt:hh\n",").\n","  - review_rating: The rating of the business.\n","  * review_text: The English review text. If the review is in another language, only the English translation is extracted, and emojis are removed. The text is normalized to lowercase.\n","  * If_pic: Whether the reviewer included pictures (Y/N).\n","  * pic_dim: The dimensions of any pictures included, as a list of tuples (e.g., [[h,w],[h,w]...]). An empty list is used if there are no pictures.\n","  * If_response: Whether the review has a response (Y/N).\n","* earliest_review_date: The earliest review date for the business in the given data subset (UTC format).\n","* latest_review_date: The latest review date for the business in the given data subset (UTC format).\n","\n","We'll continue to examine to see if we can access each element of our data"],"metadata":{"id":"utGdZO47Ch_T"}},{"cell_type":"code","source":["# Examine the structure of json file\n","data[gmap_list[0]].keys()"],"metadata":{"id":"nsxc9vlOnS9j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725020876655,"user_tz":-180,"elapsed":617,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"0175a57e-d4af-40df-c11c-1f75abdeee37"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['reviews', 'earliest_review_date', 'latest_review_date'])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"i0yElwywlDEa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725020877658,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"2380b6d0-3b4c-4616-951a-d468a86c0630"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['user_id', 'time', 'review_rating', 'review_text', 'if_pic', 'pic_dim', 'if_response'])"]},"metadata":{},"execution_count":10}],"source":["data[gmap_list[0]]['reviews'][0].keys()"]},{"cell_type":"markdown","source":["Next steps is to verify that our review_text contain no emoji and is all lowercase."],"metadata":{"id":"tCRiPJckDtcC"}},{"cell_type":"code","source":["def contains_emoji(text):\n","    emoji_pattern = re.compile(\n","        \"[\"\n","        \"\\U0001F600-\\U0001F64F\"\n","        \"\\U0001F300-\\U0001F5FF\"\n","        \"\\U0001F680-\\U0001F6FF\"\n","        \"\\U0001F1E0-\\U0001F1FF\"\n","        \"\\U00002702-\\U000027B0\"\n","        \"\\U000024C2-\\U0001F251\"\n","        \"]+\",\n","        flags=re.UNICODE\n","    )\n","    return bool(emoji_pattern.search(text))"],"metadata":{"id":"JdTlSIOEzwxD","executionInfo":{"status":"ok","timestamp":1725020878633,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Check the text reviews\n","n_issues = 0\n","\n","for gmap_id in gmap_list:\n","  reviews = data[gmap_id]['reviews']\n","  for review in reviews:\n","    if review['review_text'] is not None:\n","      # Check if all text review conatin no emoji\n","      if contains_emoji(review['review_text']):\n","        print(f\"The record for gmap_id {gmap_id} contains an emoji. Text: {review['review_text']}\")\n","        n_issues += 1\n","\n","print(f\"\\n {n_issues} has been found.\")\n","print('Done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hkF6PaSzhiZ","executionInfo":{"status":"ok","timestamp":1725020886154,"user_tz":-180,"elapsed":1305,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"8045c3ee-1d89-45b1-d24e-3d34c91e9707"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," 0 has been found.\n","Done\n"]}]},{"cell_type":"code","source":["# Check if all review_text are in lowercase (excepting 'None')\n","n_issues = 0\n","for gmap_id in gmap_list:\n","  reviews = data[gmap_id]['reviews']\n","  for review in reviews:\n","    if review['review_text'] is not None and any(char.isalpha() for char in review['review_text']):  # Check if text has alphabetic characters\n","          if not (review['review_text'].islower() or review['review_text'] == 'None'):\n","            print(f\"The record for gmap_id {gmap_id} is not in lowercase. Text: {review['review_text']}\")\n","            n_issues += 1\n","\n","print(f\"\\n {n_issues} has been found.\")\n","print('Done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cesZeMiWzoig","executionInfo":{"status":"ok","timestamp":1725021108997,"user_tz":-180,"elapsed":401,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"2c1c1f16-c3ff-4c5a-b8f9-318368e18397"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["The record for gmap_id 0x80ea13b6e203e78d:0x36861194143d1ebd is not in lowercase. Text: Øª\n","\n"," 1 has been found.\n","Done\n"]}]},{"cell_type":"markdown","source":["It is a non-English symbol"],"metadata":{"id":"0oiKemtTQ6MI"}},{"cell_type":"code","source":["# Check the text reviews\n","\n","english_symbols = r\"a-zA-Z0-9\\s\\.,;!?\\'\\\"\\-\\+\\$\\#\\:\\(\\)\\\\\\/\\â€™\\@\\&\\â€œ\\â€\\*\\%`\\-â€¦\\=\\â€”\\!\"\n","pattern_is_english = re.compile(r'^[' + english_symbols+r']+$')\n","not_english_symbols = re.compile(r'[^' + english_symbols+r']')\n","n_issues = 0\n","non_english_words = []\n","\n","for gmap_id in gmap_list:\n","  reviews = data[gmap_id]['reviews']\n","  for review in reviews:\n","    if review['review_text'] is not None:\n","      # Check if all review_text are in English\n","      if not pattern_is_english.match(review['review_text']):\n","        non_english_words.append(not_english_symbols.findall(review['review_text']))\n","        n_issues += 1\n","\n","print(non_english_words[:10])\n","print(f\"\\n {n_issues} has been found.\")\n","print('Done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCyR6Iiu1Lob","executionInfo":{"status":"ok","timestamp":1725021047862,"user_tz":-180,"elapsed":396,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"827763ca-faf9-4e39-ad7d-a2576a82d1b1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[['ðŸ¤—'], ['Ã±'], [], ['ðŸ¤™'], ['Ã©', 'Ã©'], ['ðŸ¤¤'], ['Ã£'], ['Ã»', 'Ã©'], ['ðŸ¤—'], ['Ã­']]\n","\n"," 288 has been found.\n","Done\n"]}]},{"cell_type":"markdown","source":["These symbols are non-English or emojis but not from provided list to remove"],"metadata":{"id":"HW6__8weRFvP"}},{"cell_type":"code","source":["# Check if all review_text are in English\n","for gmap_id in gmap_list:\n","    reviews = data[gmap_id]['reviews']\n","    for review in reviews:\n","        review_text = review['review_text']\n","        if review_text is not None:\n","          if len(re.findall(r'\\(original\\).*', review_text)) > 0:\n","            print(gmap_id)\n","            print(review_text)\n","print('Done')"],"metadata":{"id":"Z16l3yEdjhG6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021125225,"user_tz":-180,"elapsed":360,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"ba2b8c30-621e-4614-8f70-2419e0e8bd5b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Done\n"]}]},{"cell_type":"markdown","source":["The cells above verify that the input text is of lowercase, in English, and contains no emoji. The cell that check if the text is in English printed out some issues; however, majority of them are emojies that are not in the list we need to remove so we just leave it there. Moreover, we have over 30 thousand observation of data and only 265 observations contain problem, so it is acceptable."],"metadata":{"id":"EoKZhELJEE44"}},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ENnHWjoXlDEc"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","## 4.  Filtering and Parsing File <a class=\"anchor\" name=\"load\"></a>"]},{"cell_type":"markdown","metadata":{"id":"N9esGMx8lDEc"},"source":["In this section, we will create a vocab and countvector for review_text of gmap_id that has more than 70 reviews. We will start by filtering out gmap_id with more than 70 review_text by looking at the summary datat in csv file."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"YsnRR2c4lDEc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021140181,"user_tz":-180,"elapsed":517,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"4d6b7cec-a65c-4f53-91a0-4352e92bd306"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["135"]},"metadata":{},"execution_count":18}],"source":["#Extract gmap_id that has at least 70 text reviews\n","filt = summary_df['review_count'] >= 70\n","gmap_id_list = summary_df[filt]['gmap_id'].tolist()\n","len(gmap_id_list)"]},{"cell_type":"markdown","source":["So there are 135 businesses that has more than 70 reviews. Next we will create a dictionary where keys are gmap_id of the business and values are the corresponding review_text for that gmap_id. One business will have multiple reviews, but for processing, we will concatnate the review into 1 long string."],"metadata":{"id":"TBXZsylVyccM"}},{"cell_type":"code","source":["gmap_review_dict = {}\n","for gmap_id in gmap_id_list:\n","    review_sum = ''\n","    for review in data[gmap_id]['reviews']:\n","        review_text = review['review_text']\n","        if review_text != 'None':\n","            if review_sum:  # Check if review_sum is not empty\n","                review_sum += ' '  # Add space before the new review\n","            review_sum += review_text\n","    gmap_review_dict[gmap_id] = review_sum"],"metadata":{"id":"TjC7g-oUaech","executionInfo":{"status":"ok","timestamp":1725021151725,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Check to see if the dictionary is created correctly\n","gmap_review_dict['0x80859a3b08246ae3:0x73dd87ce4c42c354'][:500]"],"metadata":{"id":"MPXftCjaag6l","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1725021157402,"user_tz":-180,"elapsed":409,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"67bf6fb9-04a1-4677-f53d-1c2363b263a0"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"they saved mango's life.mango was diagnosed with lymphoma at another clinic and quickly went downhill after his elspar injection.  he could barely stand.  at 14 we thought his little body couldn't handle any more and we feared the worst when we brought him here.the staff was calming and professional and most important they kept us constantly informed so we were never left wondering what was going on.after his first night here we came to visit in the outside tent. honestly i thought it would be o\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["We examine the first 500 characters of one of our review_text to see how it looks. The next step is Tokenization."],"metadata":{"id":"BkXGCGME8DVy"}},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"VIfQCD1VlDEe"},"source":["<div class=\"alert alert-block alert-warning\">\n","    \n","### 4.1. Approach and Rationale <a class=\"anchor\" name=\"Approach and Rationale\"></a>"]},{"cell_type":"markdown","source":["**Orders of Processing Text and Rationalet**\n","We decided to process our text data with the following order:\n","1. Tokenization\n","2. Context-independent stopwords and short tokens removal\n","3. Rare tokens and context-dependent stopwords removal\n","4. Generate top 200 bigrams\n","5. Stemming unigram and keep bigram intact\n","6. Calculate vocabulary including both unigrams and bigrams\n","7. Create a sparse matrix and CountVector\n","\n","The rationale for that specific order of processing is as followed.\n","1. **Tokenization**: This step breaks the text into individual tokens. This is being done first because it's fundamental for all subsequent text processing steps.\n","\n","2. **Context-independent stopwords and short tokens removal**: This step removes common stopwords (such as \"and\", \"the\", etc.) and short tokens, which are assumed to be meaningless. This step is being done before **Rare token and context-dependent stopwords removal** because removing stopwords and short tokens early can reduces the dataset size quickly. These common stopwords and short tokens don't add much value to the analysis so removing them would simplies the following steps.\n","\n","3. **Rare token and context-dependent stopwords removal**: Rare tokens and context-dependent stopwords are removed to further clean the data. Rare tokens and context-dependent stopwords are defined as having document frequency of less than 5% and more than 95% respectively. Rare tokens can introduce noise while context-dependent stopwords (words that appear in almost every document) don't provide meaningful information for analysis.\n","\n","4. **Generating top 200 bigrams**: Top 200 bigrams based on Pointwise Mutual Information (PMI) are added into the list of tokens.\n","\n","**Remarks:** Stopwords and rare tokens removal was done before generating bigrams to prevent noise in bigrams and reduce the computing load. Stopwords and rare tokens don't have much meaning and if they were kept when generating bigrams, meaningless bigrams like \"of the\", \"and it\" would be generated. Removing stopwords and rare tokens early on also reduce the number of potential bigrams.\n","\n","5. **Stemming unigram and keep bigram intact**: This step stems unigram to its root form using PorterStemmer. Bigrams are kept intact to preserve the specific word combinations that could lose meaning if stemmed.\n","\n","**Remarks:** Stemming was done after stopwords (context-dependent and independent) removal to avoid stemming irrelevant words and altering the document frequency of some words. Since stemming return words to its root form, some words may experience an increased in document frequency compare to its original forms. For example, the words \"run\", \"running\", and \"runs\" which has document frequency 1 in three documents, after stemming to \"run\", the word \"run\" will have document frequency of 3. Context-dependent stopwords and rare tokens are defined based on document frequency, thus with changed document frequency, removing context-dependent and rare tokens would be inaccurate.\n","\n","6. **Calculate vocabulary including both unigrams and bigrams**: Combining the cleaned and processed unigrams and bigrams into a final vocabulary list ensures that your text analysis captures both individual word meanings and important word combinations.\n","\n","7. **Create a sparse matrix and CountVector**: Finally, the creation of a sparse matrix using CountVectorizer transforms the vocabulary into a numerical format, ready for machine learning models or other quantitative analysis.\n","\n","The details of each steps are below.\n"],"metadata":{"id":"XYupCbAY044C"}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-warning\">\n","    \n","### 4.2. Tokenization and Stopwords Removal <a class=\"anchor\" name=\"Tokenization and Stopwords Removal\"></a>"],"metadata":{"id":"mQv2SP_TXB4u"}},{"cell_type":"markdown","metadata":{"id":"gchByyjolDEf"},"source":["Tokenization is a principal step in text processing and producing unigrams. In this section, we will turn review_text into a list of tokens for each business gmap_id. Then, context-independent stopwords and short tokens (of length less than three) will be removed. Our output would be a dictionary where each key is a gmap_id and each value is a corresponding list of tokens for that gmap_id (context-dependent stopwords and short tokens of length less than 3 removed).\n","\n"]},{"cell_type":"code","source":["#Stopwords\n","ilya_path = \"/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1/stopwords_en.txt\"\n","tung_path = \"/content/drive/MyDrive/FIT5196_assignment_files/stopwords_en.txt\"\n","with open(ilya_path, 'r') as file:\n","    context_independent_stopwords_list = file.read().splitlines()"],"metadata":{"id":"Awn3ItAnalOA","executionInfo":{"status":"ok","timestamp":1725021167676,"user_tz":-180,"elapsed":396,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["After having a list of context-independent stopwords, we will loop through each gmap_id and review_text in our gmap_review_dict and tokenize each review_text using RegexpTokenizer with regular expression r[a-zA-Z]+\". After that we'll remove context-independent stopwords and tokens with length less than 3 using Python list comprehension. Finally, we will store our list of tokens for each gmap_id into a new dictionary, gmap_id_token_dict."],"metadata":{"id":"U79Ipn0jYaMN"}},{"cell_type":"code","source":["gmap_id_token_dict = {}\n","for gmap_id, review_text in gmap_review_dict.items():\n","    # Tokenize the review text\n","    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n","    unigram_tokens = tokenizer.tokenize(review_text)\n","\n","    #Remove context-independent stopwords and tokens with length < 3\n","    unigram_tokens = [token for token in unigram_tokens if token.lower() not in context_independent_stopwords_list]\n","    unigram_tokens = [token for token in unigram_tokens if len(token) >= 3]\n","\n","    #Store token list to a corresponding gmap_id\n","    gmap_id_token_dict[gmap_id] = unigram_tokens\n","\n","len(gmap_id_token_dict['0x80859a3b08246ae3:0x73dd87ce4c42c354'])"],"metadata":{"id":"Y18VQ6FEaqTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021175742,"user_tz":-180,"elapsed":4924,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"6286d7e9-da60-4947-c292-98037536e480"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5056"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-warning\">\n","\n","### 4.3. Generating top 200 bigrams <a class=\"anchor\" name=\"Generating top 200 bigrams\"></a>"],"metadata":{"id":"wmx7QS49dxat"}},{"cell_type":"markdown","source":["First, we will generate a list of all tokens available from our review_text, all_tokens_list, using chain.from_iterable(), then we create a unique vocabulary by create a set of all_tokens_list.\n","\n"],"metadata":{"id":"3H_WDtj7eqbW"}},{"cell_type":"code","source":["all_tokens_list = list(chain.from_iterable(gmap_id_token_dict.values()))\n","len(all_tokens_list)"],"metadata":{"id":"Fi_y69G7avrm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021182445,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"31966b8b-ac83-48db-e6a9-e70e715f42dd"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["245552"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["uni_voc = list(set(all_tokens_list))\n","len(uni_voc)"],"metadata":{"id":"razUUfhOaw7K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021185714,"user_tz":-180,"elapsed":401,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"525db18a-5f35-4a9c-c53a-a608dd0ecb47"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16320"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["We generate bigrams using nltk.collocations.BigramAssocMeasures() and apply to our all_tokens_list. Next, top 200 bigrams by PMI is filtered."],"metadata":{"id":"Dpu3obBMf_Vr"}},{"cell_type":"code","source":["#generate top 200 bigrams\n","bigram_measures = nltk.collocations.BigramAssocMeasures()\n","finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens_list)\n","top_200_bigrams = finder.nbest(bigram_measures.pmi, 200)\n","top_200_bigrams[:10]\n"],"metadata":{"id":"Rl_glyM-ayO2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021190557,"user_tz":-180,"elapsed":2851,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"0b0db47a-adef-41fd-f147-a29b6cebf869"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('aarrrggh', 'corporations'),\n"," ('abbot', 'kenny'),\n"," ('abusive', 'creeps'),\n"," ('accountability', 'instilling'),\n"," ('accounted', 'attatude'),\n"," ('accurated', 'calculations'),\n"," ('achievable', 'empowering'),\n"," ('administrational', 'therapeutic'),\n"," ('afb', 'ventured'),\n"," ('affectation', 'refined')]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["We then check if our bigrams are actually appear in our review_text by re-tokenize them using MWETokenizer()\n"],"metadata":{"id":"eQCzbouZgzpx"}},{"cell_type":"code","source":["uni_voc.extend(top_200_bigrams)\n","from nltk.tokenize import MWETokenizer\n","tokenizer = MWETokenizer(uni_voc)\n","mwe_tokens = tokenizer.tokenize(all_tokens_list)\n","len(mwe_tokens)"],"metadata":{"id":"epVqqw6Ba2Ec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021213625,"user_tz":-180,"elapsed":777,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"7f774e1c-90f8-43fb-8c2d-ecc4557c0214"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["245362"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["mwe_tokens = list(set(mwe_tokens))\n","len(mwe_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXCzGk1aeuhN","executionInfo":{"status":"ok","timestamp":1725021215391,"user_tz":-180,"elapsed":400,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"e6d90b1c-b5fe-4001-c7c5-9e768ea26a0b"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16130"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# Check if we have bigrams in our list\n","bigram_checklist = []\n","for bigram in mwe_tokens:\n","  if '_' in bigram:\n","    bigram_checklist.append(bigram)\n","\n","# Inspect the output\n","len(bigram_checklist)"],"metadata":{"id":"18Mru6PusTHS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021218050,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"03e41db8-a4d8-41bc-c425-35b795a1dc80"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["190"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["So we have added 190 bigrams into our vocab."],"metadata":{"id":"Jm_G6K0bLwTt"}},{"cell_type":"markdown","source":["Now we remove tokens with length less than 3."],"metadata":{"id":"oiLNYxX_L34A"}},{"cell_type":"code","source":["mwe_tokens = [token for token in mwe_tokens if len(token) >= 3]\n","len(mwe_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHCa_y3mbafz","executionInfo":{"status":"ok","timestamp":1725021222240,"user_tz":-180,"elapsed":432,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"a0664017-c288-4c99-90ad-b56a02808fbf"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16130"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-warning\">\n","\n","### 4.3. Context-dependent stopwords and rare tokens <a class=\"anchor\" name=\"Approach and Rationale\"></a>\n"],"metadata":{"id":"2bEFknF3Xu54"}},{"cell_type":"markdown","source":["The next step is to remove context-dependent stopwords and rare tokens. Context-dependent stopwords and rare tokens are defined using document frequency of words, so we will create a document frequency for each words in our vocab.\n","\n","First, we create a set of tokens for each gmap_id_token_dict values. The set operation would make each word appear once only in each review, thus when combine all the review and count the number of appearance for each tokens, we will get the document frequency for each word. Combining set(value) for value in gmap_id_token_dict.values() is done using chain.from_iterable, and counting appearance of each tokens is done by FreqDist() function from nltk.probability.\n","\n","The threshhold for words removal is set at less than 5% and more than 95% as specified in the assignment requirements.\n","\n","Then, we remove tokens from vocab."],"metadata":{"id":"iBVgVDosZovF"}},{"cell_type":"code","source":["words = list(chain.from_iterable([set(value) for value in gmap_id_token_dict.values()]))\n","fd = FreqDist(words)\n","\n","#Set up threshhold for inclusion criteria\n","num_businesses = len(gmap_review_dict)\n","upper_threshhold = 0.95 * num_businesses\n","lower_threshhold = 0.05 * num_businesses\n","\n","context_dependent_stopwords_list = set([word for word, freq in fd.items() if freq > upper_threshhold])\n","rare_tokens = set([word for word, freq in fd.items() if freq < lower_threshhold])\n","\n","mwe_tokens = [token for token in mwe_tokens if token not in context_dependent_stopwords_list and token not in rare_tokens]\n","len(mwe_tokens)"],"metadata":{"id":"i0XzJHKkatGg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021225766,"user_tz":-180,"elapsed":371,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"7b7bccd3-095f-4226-eac7-182b6757857a"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2729"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["We printed out the number of tokens in a our vocab after removing context-dependent stopwords and rare tokens to see if our code works."],"metadata":{"id":"ieUOEA9tc4ds"}},{"cell_type":"markdown","source":["So, our tokens list contain both unigrams and bigrams we need. The next step is to stem unigrams using PorterStemmer()"],"metadata":{"id":"RuLhzorIMEk6"}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-warning\">\n","    \n","### 4.5. Stemming, create vocabulary, and output to vocab.txt <a class=\"anchor\" name=\"Stemming\"></a>"],"metadata":{"id":"JkZbOAVEuCYt"}},{"cell_type":"markdown","source":["We will stem unigrams and preserve bigrams using PorterStemmer(). Bigrams are kept intact to preserve the specific word combinations that could lose meaning if stemmed.\n","\n","Firstly, we write a function that stem unigrams only, then we apply that function to out list of mwe_tokens. Finally, we use set() so that each tokens appear only once in our list."],"metadata":{"id":"saiDpGFQRgF0"}},{"cell_type":"code","source":["stemmer = PorterStemmer()\n","\n","def stem_unigrams_only(tokens):\n","    stemmed_tokens = []\n","    for token in tokens:\n","        if \"_\" in token:  # Skip bigrams (containing \"_\")\n","            stemmed_tokens.append(token)\n","        else:  # Stem unigrams\n","            stemmed_tokens.append(stemmer.stem(token))\n","    return stemmed_tokens\n","\n","stemmed_tokens = stem_unigrams_only(mwe_tokens)\n","\n","# Inspect the output\n","stemmed_tokens[:10]"],"metadata":{"id":"wuYYDoTTa4uv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021232532,"user_tz":-180,"elapsed":550,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"fbdfbe47-cd75-4c8f-f885-2e476c2992df"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['paid',\n"," 'appar',\n"," 'spanish',\n"," 'abbot_kenny',\n"," 'slice',\n"," 'form',\n"," 'patron',\n"," 'mask',\n"," 'adult',\n"," 'grate']"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["len(stemmed_tokens)"],"metadata":{"id":"ZfhtrsOTa6ia","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021235591,"user_tz":-180,"elapsed":380,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"fcd0b934-d362-475e-e171-d7f64f3efa54"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2729"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# Create a unique and sorted vocab list\n","stemmed_tokens = list(set(stemmed_tokens))\n","stemmed_tokens = sorted(stemmed_tokens)\n","len(stemmed_tokens)"],"metadata":{"id":"i5i17RnAa8X6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021237474,"user_tz":-180,"elapsed":385,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"99559654-f474-4b77-efc0-ba1da422814e"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1991"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["We have a vocab, stemmed_token, that contains both bigrams and unigrams. We have remove context-dependent, context-independent stopwords, short tokens, and rare tokens. We will now write them to our output file."],"metadata":{"id":"G9zR0HodM8xv"}},{"cell_type":"code","source":["with open('/content/drive/MyDrive/FIT5196_assignment_files/044_vocab.txt', 'w') as f:\n","    for index, word in enumerate(stemmed_tokens, start=0):\n","        f.write(f\"{word}:{index}\\n\")"],"metadata":{"id":"EoEI4TSNa-PL","executionInfo":{"status":"ok","timestamp":1725021243607,"user_tz":-180,"elapsed":549,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-warning\">\n","    \n","### 4.6. Create a sparse and CountVector <a class=\"Create a sparse\" name=\"whetev\"></a>"],"metadata":{"id":"yzVG6HMSTS_s"}},{"cell_type":"markdown","source":["The final step is to create a sparse matrix and its countvector using CountVectorizer().\n","\n","Because we created new bigrams but we haven't added them into the token list corresponding to each of our gmap_id, we have to add them before we create a count vector."],"metadata":{"id":"B1d4g7nDT7I6"}},{"cell_type":"code","source":["# Add bigrams into corresponding gmap_id_token_dict, re-tokenize, and stem unigrams\n","\n","for gmap_id, tokens in gmap_id_token_dict.items():\n","    # Generate bigrams for each gmap_id\n","    uni_voc = list(set(tokens))\n","    uni_voc.extend(top_200_bigrams)\n","\n","    # Create a new list of tokens\n","    tokenizer = MWETokenizer(uni_voc)\n","    mwe_tokens = tokenizer.tokenize(tokens)\n","\n","    # Update the list of tokens\n","    gmap_id_token_dict[gmap_id] = mwe_tokens\n","\n","    # Stem unigrams for each gmap_id\n","    gmap_id_token_dict[gmap_id] = stem_unigrams_only(gmap_id_token_dict[gmap_id])"],"metadata":{"id":"S6dKpI0sbAPx","executionInfo":{"status":"ok","timestamp":1725021258679,"user_tz":-180,"elapsed":8434,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["We create a mapping between token and index number to match the expected output."],"metadata":{"id":"ZJdlCEhuU28S"}},{"cell_type":"code","source":["vocab = stemmed_tokens\n","token_to_index = {token: index for index, token in enumerate(vocab)}"],"metadata":{"id":"SaLkfVJhbBGB","executionInfo":{"status":"ok","timestamp":1725021259981,"user_tz":-180,"elapsed":1,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["gmap_list = list(gmap_id_token_dict.keys())\n","gmap_list[:10]"],"metadata":{"id":"wZGUEqZKbFwG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021262560,"user_tz":-180,"elapsed":350,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"41f21bcb-eb8e-4f1b-c6d9-32b7aa28e750"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['0x808164de5730862d:0xcb473d5d65fb1d39',\n"," '0x808327ba5f197e6d:0x5e31fb9492a334e7',\n"," '0x8084047107a176cb:0x889336c7010bad47',\n"," '0x8084acae2f9d2541:0xe64c0a74ef6732af',\n"," '0x8084d092a8afc95d:0x1b7c5c974eb7e63f',\n"," '0x8084d0e3d7fa160f:0xf481ebe2fa126e60',\n"," '0x8085072c051576f1:0xef004a4a10bf4322',\n"," '0x808566df363f8e6b:0x4981b6aa71d7df9',\n"," '0x80857342c8df43f5:0x63ca2b246ed1de5e',\n"," '0x8085778fd5a0ea71:0xe9c5b4b613568661']"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["We instantiated a CountVectorizer() instance with vocabulary is our vocab and created a sparse matrix."],"metadata":{"id":"k1pjZUWbVGRC"}},{"cell_type":"code","source":["vectorizer = CountVectorizer(vocabulary=vocab)\n","text = [' '.join(tokens) for tokens in gmap_id_token_dict.values()]\n","X = vectorizer.fit_transform(text)"],"metadata":{"id":"eCUM5mqVbHcc","executionInfo":{"status":"ok","timestamp":1725021265165,"user_tz":-180,"elapsed":351,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Inspect the matrix\n","X.shape"],"metadata":{"id":"Xxgd3SdlbI_C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021266197,"user_tz":-180,"elapsed":564,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"d98191d6-ff3b-4302-d073-093330625fe9"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(135, 1991)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# Convert matrix to array for access\n","X_arr = X.toarray()\n","X_arr"],"metadata":{"id":"rRpHVu8tbKSe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725021267660,"user_tz":-180,"elapsed":384,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}},"outputId":"bbce031b-d0db-4d3d-f06d-c6df67195be4"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 2, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 1, 2, 0]])"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["Now we have a matrix where each rows is a gmap_id and each column is how many times a specific word appear in that document. Knowing the structure of the matrix, we will start writing our output"],"metadata":{"id":"gDjNGh5rVgro"}},{"cell_type":"code","source":["with open('/content/drive/MyDrive/FIT5196_assignment_files/044_countvec.txt', 'w') as file:\n","  for i, row in enumerate(X_arr):\n","    line = gmap_list[i]\n","    for j, val in enumerate(row):\n","      if val > 0:\n","        line += f\",{token_to_index[vocab[j]]}:{val}\"\n","    file.write(line + '\\n')"],"metadata":{"id":"WWDUBxCrbMEk","executionInfo":{"status":"ok","timestamp":1725021269156,"user_tz":-180,"elapsed":465,"user":{"displayName":"Ilya Bessonov","userId":"10923773388509383201"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWjri6x_lDEn"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"]},{"cell_type":"markdown","metadata":{"id":"RXAprlSblDEn"},"source":["In this notebook, we successfully processed Google Reviews data to extract meaningful features that are crucial for various downstream natural language processing tasks, such as recommender systems, information retrieval, and machine translation. The preprocessing pipeline involved several key steps:\n","\n","1. Data Loading and Initial Exploration: We started by loading data from a CSV file containing business-related information and a JSON file containing detailed review data. We ensured that the review texts were normalized, free of emojis, and in English.\n","\n","2. Text Processing: The core text processing tasks were systematically executed, including tokenization, stopwords removal, and stemming. We also identified and removed rare tokens and context-dependent stopwords to clean the dataset effectively.\n","\n","3. Bigram Generation and Integration: We generated the top 200 bigrams based on Pointwise Mutual Information (PMI) and integrated these into our vocabulary, maintaining the meaning of specific word combinations.\n","\n","4. Vocabulary Creation and Sparse Matrix Generation: A final vocabulary list, comprising both unigrams and bigrams, was generated. Using this vocabulary, we created a sparse matrix and a corresponding CountVector, which were outputted for further analysis.\n","\n","Overall, the structured and detailed approach taken in this notebook provided a solid foundation for further text analysis, ensuring that the processed data is in a format suitable for machine learning models and other quantitative analyses."]},{"cell_type":"markdown","metadata":{"id":"HppxDtWNlDEn"},"source":["<div class=\"alert alert-block alert-success\">\n","    \n","## 7. References <a class=\"anchor\" name=\"Ref\"></a>"]},{"cell_type":"markdown","metadata":{"id":"PCkWr-M1lDEo"},"source":["[1] Pandas DataFrame: Wes McKinney, â€œpandas: powerful Python data analysis toolkit,â€ https://pandas.pydata.org/, Accessed 24/08/2024.\n","\n","[2] NLTK Tokenization and Stemming: Steven Bird, Ewan Klein, and Edward Loper, â€œNatural Language Toolkit,â€ http://www.nltk.org/, Accessed 24/08/2024.\n","\n","[3] CountVectorizer (scikit-learn): Pedregosa et al., â€œScikit-learn: Machine Learning in Python,â€ Journal of Machine Learning Research, vol. 12, pp. 2825-2830, 2011, https://scikit-learn.org/, Accessed 24/08/2024.\n","\n","[4] RegexpTokenizer (NLTK): Bird, S., Klein, E., & Loper, E. (2009). â€œNatural Language Processing with Python,â€ Oâ€™Reilly Media Inc. http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.RegexpTokenizer, Accessed 24/08/2024.\n","\n","[5] MWETokenizer (NLTK): Bird, S., Klein, E., & Loper, E. (2009). â€œNatural Language Processing with Python,â€ Oâ€™Reilly Media Inc. http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.mwe.MWETokenizer, Accessed 24/08/2024.\n","\n","[6] FreqDist (NLTK): Bird, S., Klein, E., & Loper, E. (2009). â€œNatural Language Processing with Python,â€ Oâ€™Reilly Media Inc. http://www.nltk.org/api/nltk.probability.html#nltk.probability.FreqDist, Accessed 24/08/2024.\n","\n","[7] PorterStemmer (NLTK): Bird, S., Klein, E., & Loper, E. (2009). â€œNatural Language Processing with Python,â€ Oâ€™Reilly Media Inc. http://www.nltk.org/api/nltk.stem.html#nltk.stem.PorterStemmer, Accessed 24/08/2024.\n","\n","We acknowledge the use of Gen-AI (ChatGPT, Copylot and Gemini) to get some hints about specific Python libraries/functions/classes/methods/options/settings. Every information from AI has been found and double-checked in the original library manuals.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mp9O-a1UlDEo"},"source":["## --------------------------------------------------------------------------------------------------------------------------"]}],"metadata":{"colab":{"provenance":[{"file_id":"1XLZPY2q6RntuPoGWWHrqGErRT-ASZmUr","timestamp":1724852002909}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":0}